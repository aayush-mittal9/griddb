{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting flight delays using Python and GridDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the associated financial losses that the aviation industry is constantly experiencing, flight delays have become a very important topic for air transportation all over the world.\n",
    "\n",
    "The causes of these delays are many and diverse, ranging from air traffic congestion to weather conditions, mechanical issues, challenges with passenger boarding, and simply the airlines' inability to meet demand given their capacity.\n",
    "\n",
    "Based on historical data of flight delays, we will first analyse the reasons for delays and then we will predict flight delay time prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outline of the tutorial is as follows:\n",
    "\n",
    " 1. Dataset overview\n",
    " 2. Importing required libraries\n",
    " 3. Loading the dataset\n",
    " 4. Data Preporcessing and data visualization\n",
    " 5. Predictions\n",
    " 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites and Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is carried out in Anaconda Navigator (Python version – 3.8.5) on Windows Operating System. The following packages need to be installed before you continue with the tutorial –\n",
    "\n",
    "1. Pandas\n",
    "\n",
    "2. NumPy\n",
    "\n",
    "3. sklearn\n",
    "\n",
    "4. Matplotlib\n",
    "\n",
    "5. Seaborn\n",
    "\n",
    "6. statsmodels\n",
    "\n",
    "You can install these packages in Conda’s virtual environment using `conda install package-name`. In case you are using Python directly via terminal/command prompt, `pip install package-name` will do the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridDB installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While loading the dataset, this tutorial will cover two methods – Using GridDB as well as Using Pandas. To access GridDB using Python, the following packages also need to be installed beforehand:\n",
    "\n",
    "1. [GridDB C-client](https://github.com/griddb/c_client)\n",
    "2. SWIG (Simplified Wrapper and Interface Generator)\n",
    "3. [GridDB Python Client](https://github.com/griddb/python_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Overview\n",
    "\n",
    "The Bureau of Transportation Statistics of the United States Department of Transportation (DOT) monitors the on-time performance of domestic flights operated by large air carriers. The DOT's monthly Air Travel Consumer Report and this dataset of 2015 flight delays and cancellations contain summary information on the number of on-time, delayed, cancelled, and diverted flights.\n",
    "\n",
    "https://www.kaggle.com/code/manasichhibber/flight-delay-predictions/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import requests\n",
    "import http\n",
    "http.client.HTTPConnection.debuglevel = 1\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "import numpy as np   \n",
    "import statsmodels.formula.api as sm\n",
    "import datetime\n",
    "import time\n",
    "from time import strftime, gmtime\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "from sklearn import ensemble,gaussian_process,linear_model,naive_bayes,neighbors,svm,tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score,precision_score,recall_score,auc\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVC\n",
    "from random import sample\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loading the Dataset\n",
    "\n",
    "Toshiba GridDB™ is a highly scalable NoSQL database best suited for IoT and Big Data. The foundation of GridDB’s principles is based upon offering a versatile data store that is optimized for IoT, provides high scalability, tuned for high performance, and ensures high reliability.\n",
    "\n",
    "To store large amounts of data, a CSV file can be cumbersome. GridDB serves as a perfect alternative as it in open-source and a highly scalable database. GridDB is a scalable, in-memory, No SQL database which makes it easier for you to store large amounts of data. If you are new to GridDB, a tutorial on  [reading and writing to GridDB](https://griddb.net/en/blog/using-pandas-dataframes-with-griddb/)  can be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Container in GridDB to store the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this blog post is to showcase the noteworthy features of GridDB, including:\n",
    "\n",
    "- Retrieving data using SQL with sub-queries, case statements, group by clauses, having clauses and nested sub-queries.\n",
    "- Exploring the two container types offered by GridDB: Collections and Time Series. For more information on GridDB data modeling, read this article. (https://griddb.net/en/blog/data-modeling-with-griddb)\n",
    "\n",
    "Get started by subscribing to GridDB Cloud. (https://griddb.net/en/blog/an-introduction-to-griddb-cloud/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file ‘container_columns’ has all the headers and their datatypes. One last step is to change the Python datatypes to GridDB datatypes. Click here (https://docs.griddb.net/tqlreference/type/#primitive) to learn more about the datatypes supported by GridDB and their respective notations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = pd.read_csv('flights.csv')\n",
    "\n",
    "# Extracting column headers for the GridDB WebApi Container structure\n",
    "col_vals = (list(flights.columns.values))\n",
    "col_dtypes = (list(flights.dtypes))\n",
    "data_tuples = list(zip(col_vals,col_dtypes)) # Binding lists to a tuple\n",
    "\n",
    "# Converting list of tuples to pandas dataframe\n",
    "container_columns1 = pd.DataFrame(data_tuples, columns=['col_vals','col_datatypes'])\n",
    "container_columns1.to_csv('container_columns1.csv',index=False)\n",
    "container_columns1 = container_columns1.dropna(how='any',axis=0)\n",
    "container_columns1 #column names for the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = pd.read_csv('airlines.csv')\n",
    "\n",
    "col_vals = (list(airlines.columns.values))\n",
    "col_dtypes = (list(airlines.dtypes))\n",
    "data_tuples = list(zip(col_vals,col_dtypes)) \n",
    "\n",
    "container_columns2 = pd.DataFrame(data_tuples, columns=['col_vals','col_datatypes'])\n",
    "container_columns2.to_csv('container_columns2.csv',index=False)\n",
    "container_columns2 = container_columns2.dropna(how='any',axis=0)\n",
    "\n",
    "container_columns2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = pd.read_csv('airports.csv')\n",
    "\n",
    "col_vals = (list(airports.columns.values))\n",
    "col_dtypes = (list(airports.dtypes))\n",
    "data_tuples = list(zip(col_vals,col_dtypes))\n",
    "\n",
    "container_columns3 = pd.DataFrame(data_tuples, columns=['col_vals','col_datatypes'])\n",
    "container_columns3.to_csv('container_columns3.csv',index=False)\n",
    "container_columns3 = container_columns3.dropna(how='any',axis=0)\n",
    "\n",
    "container_columns3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ = {'int64': 'INTEGER', 'float64': 'FLOAT', 'object': 'STRING'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_columns1['col_datatypes'] = container_columns1['col_datatypes'].map(typ)\n",
    "container_columns2['col_datatypes'] = container_columns2['col_datatypes'].map(typ)\n",
    "container_columns3['col_datatypes'] = container_columns3['col_datatypes'].map(typ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a Container in GridDB to store the data. To learn more about creating a container in GridDB, refer to this resource of GridDB. (https://docs.griddb.net/architecture/data-model/#container)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct an object to hold the request headers (ensure that you replace the XXX placeholder with the correct value that matches the credentials for your GridDB instance)\n",
    "header_obj = {\"Authorization\":\"[Auth-token]\",\"Content-Type\":\"application/json; charset=UTF-8\",\"User-Agent\":\"PostmanRuntime/7.29.0\"}\n",
    "\n",
    "#Construct the base URL based on your GridDB cluster you'd like to connect to (ensure that you replace the placeholders in the URL below with the correct values that correspond to your GridDB instance)\n",
    "base_url = 'https://[host]:[port]/griddb/v2/[clustername]/dbs/[database_name]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = []\n",
    "c2 = []\n",
    "c3 = []\n",
    "\n",
    "class create_dict(dict):\n",
    "\n",
    "    def __init__(self):\n",
    "        self = dict()\n",
    "\n",
    "    def add(self, key, value):\n",
    "        self[key] = value\n",
    "\n",
    "\n",
    "def format_to_dic(a,b):\n",
    "    dct = create_dict()\n",
    "    dct.add('name', a)\n",
    "    dct.add('type', b)\n",
    "\n",
    "    return dct\n",
    "\n",
    "\n",
    "for i in range(len(container_columns1)):\n",
    "    c1.append(format_to_dic(container_columns1.iloc[i,0], container_columns1.iloc[i,1]))\n",
    "    \n",
    "for i in range(len(container_columns2)):\n",
    "    c2.append(format_to_dic(container_columns2.iloc[i,0], container_columns2.iloc[i,1]))\n",
    "    \n",
    "for i in range(len(container_columns3)):\n",
    "    c3.append(format_to_dic(container_columns3.iloc[i,0], container_columns3.iloc[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Construct an object to hold the request body (i.e., the container that needs to be created)\n",
    "data_obj1 = {\n",
    "    \"container_name\": \"flights\",\n",
    "    \"container_type\": \"COLLECTION\",\n",
    "    \"rowkey\": False,\n",
    "    \"columns\": c1\n",
    "}\n",
    "\n",
    "data_obj2 = {\n",
    "    \"container_name\": \"airlines\",\n",
    "    \"container_type\": \"COLLECTION\",\n",
    "    \"rowkey\": False,\n",
    "    \"columns\": c2 \n",
    "}\n",
    "\n",
    "data_obj3 = {\n",
    "    \"container_name\": \"airports\",\n",
    "    \"container_type\": \"COLLECTION\",\n",
    "    \"rowkey\": False,\n",
    "    \"columns\": c3\n",
    "}\n",
    "\n",
    "#Set up the GridDB WebAPI URL\n",
    "url = base_url + '/containers'\n",
    "\n",
    "#Invoke the GridDB WebAPI with the headers and the request body\n",
    "x1 = requests.post(url, json = data_obj1, headers = header_obj,auth=HTTPBasicAuth([username], [password]))\n",
    "x2 = requests.post(url, json = data_obj2, headers = header_obj,auth=HTTPBasicAuth([username], [password]))\n",
    "x3 = requests.post(url, json = data_obj3, headers = header_obj,auth=HTTPBasicAuth([username], [password]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data into the GridDB container (Row Registration)\n",
    "To learn more on the format required to add rows to the container, refer to this article. (http://www.toshiba-sol.co.jp/en/pro/griddb/docs-en/v4_3/GridDB_Web_API_Reference.html#%E3%83%AD%E3%82%A6%E7%99%BB%E9%8C%B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights.to_json(orient='values')\n",
    "request_body1 = flights\n",
    "\n",
    "airlines = airlines.to_json(orient='values')\n",
    "request_body2 = airlines\n",
    "\n",
    "airports = airports.to_json(orient='values')\n",
    "request_body3 = airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the URL to be used to invoke the GridDB WebAPI to register rows in the container created previously\n",
    "url1 = base_url + '/containers/flights/rows'\n",
    "url2 = base_url + '/containers/airlines/rows'\n",
    "url3 = base_url + '/containers/airports/rows'\n",
    "\n",
    "\n",
    "#Invoke the GridDB WebAPI using the request constructed\n",
    "x1 = requests.put(url1, data=request_body1, headers=header_obj,auth=HTTPBasicAuth([username], [password]))\n",
    "x2 = requests.put(url2, data=request_body2, headers=header_obj,auth=HTTPBasicAuth([username], [password]))\n",
    "x3 = requests.put(url3, data=request_body3, headers=header_obj,auth=HTTPBasicAuth([username], [password]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct the SQL to be used\n",
    "sql1 = (\"SELECT * FROM flights\")\n",
    "sql2 = (\"SELECT * FROM airlines\")\n",
    "sql3 = (\"SELECT * FROM airports\")\n",
    "\n",
    "url = base_url + '/sql'\n",
    "request_body1 = '[{\"type\":\"sql-select\", \"stmt\":\"'+sql1+'\"}]'\n",
    "request_body2 = '[{\"type\":\"sql-select\", \"stmt\":\"'+sql2+'\"}]'\n",
    "request_body3 = '[{\"type\":\"sql-select\", \"stmt\":\"'+sql3+'\"}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the GridDB SQL API to retrieve the results and process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_req1 = requests.post(url, data=request_body1, headers=header_obj,auth=HTTPBasicAuth([username], [password]))\n",
    "data_req2 = requests.post(url, data=request_body2, headers=header_obj,auth=HTTPBasicAuth([username], [password]))\n",
    "data_req3 = requests.post(url, data=request_body3, headers=header_obj,auth=HTTPBasicAuth([username], [password]))\n",
    "\n",
    "\n",
    "myJson1 = data_req1.json()\n",
    "flights = pd.DataFrame(myJson1[0][\"results\"], columns=[myJson1[0][\"columns\"][0][\"name\"], myJson1[0][\"columns\"][1][\"name\"]])\n",
    "\n",
    "myJson2 = data_req2.json()\n",
    "airlines = pd.DataFrame(myJson2[0][\"results\"], columns=[myJson2[0][\"columns\"][0][\"name\"], myJson2[0][\"columns\"][1][\"name\"]])\n",
    "\n",
    "myJson3 = data_req1.json()\n",
    "airports = pd.DataFrame(myJson3[0][\"results\"], columns=[myJson3[0][\"columns\"][0][\"name\"], myJson3[0][\"columns\"][1][\"name\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to IoT and Big Data use cases, GridDB clearly stands out among other databases in the Relational and NoSQL space.\n",
    "Overall, GridDB offers multiple reliability features for mission-critical applications that require high availability and data retention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preprocessing and Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various Plots are created so as to get a great idea of whats happening in the Dataset and what is the most important variable affecting the dalays of the airlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights = flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to datetime.\n",
    "def conv_time(time_val):\n",
    "    if pd.isnull(time_val):\n",
    "        return np.nan\n",
    "    else:\n",
    "        if time_val == 2400: time_val = 0\n",
    "        time_val = \"{0:04d}\".format(int(time_val))\n",
    "        time_formatted = datetime.time(int(time_val[0:2]), int(time_val[2:4]))\n",
    "    return time_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights['ARRIVAL_TIME'] = df_flights['ARRIVAL_TIME'].apply(conv_time)\n",
    "df_flights['DEPARTURE_TIME'] = df_flights['DEPARTURE_TIME'].apply(conv_time)\n",
    "df_flights['SCHEDULED_DEPARTURE'] = df_flights['SCHEDULED_DEPARTURE'].apply(conv_time)\n",
    "df_flights['WHEELS_OFF'] = df_flights['WHEELS_OFF'].apply(conv_time)\n",
    "df_flights['WHEELS_ON'] = df_flights['WHEELS_ON'].apply(conv_time)\n",
    "df_flights['SCHEDULED_ARRIVAL'] = df_flights['SCHEDULED_ARRIVAL'].apply(conv_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've converted the necessary time values to a DateTime datatype, we need to validate our data. Null values and missing data are common data states that must be handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights['AIRLINE_DELAY'] = df_flights['AIRLINE_DELAY'].fillna(0)\n",
    "df_flights['AIR_SYSTEM_DELAY'] = df_flights['AIR_SYSTEM_DELAY'].fillna(0)\n",
    "df_flights['SECURITY_DELAY'] = df_flights['SECURITY_DELAY'].fillna(0)\n",
    "df_flights['LATE_AIRCRAFT_DELAY'] = df_flights['LATE_AIRCRAFT_DELAY'].fillna(0)\n",
    "df_flights['WEATHER_DELAY'] = df_flights['WEATHER_DELAY'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights['CANCELLATION_REASON'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the primary reason for cancellation is B the weather. Weather is well known to be a major cause of delays and cancellations. In the case of this attribute, we consider the weather to be a cancellation reason rather than a delay reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting categoric value to numeric\n",
    "df_flights.loc[df_flights['CANCELLATION_REASON'] == 'A', 'CANCELLATION_REASON'] = 1\n",
    "df_flights.loc[df_flights['CANCELLATION_REASON'] == 'B', 'CANCELLATION_REASON'] = 2\n",
    "df_flights.loc[df_flights['CANCELLATION_REASON'] == 'C', 'CANCELLATION_REASON'] = 3\n",
    "df_flights.loc[df_flights['CANCELLATION_REASON'] == 'D', 'CANCELLATION_REASON'] = 4\n",
    "\n",
    "# converting NaN to zero\n",
    "df_flights['CANCELLATION_REASON'] = df_flights['CANCELLATION_REASON'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights = df_flights.dropna(axis=0)\n",
    "df_flights.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airlines = airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights = df_flights.merge(df_airlines, left_on='AIRLINE', right_on='IATA_CODE', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping old column and rename new one\n",
    "df_flights = df_flights.drop(['AIRLINE_x','IATA_CODE'], axis=1)\n",
    "df_flights = df_flights.rename(columns={\"AIRLINE_y\":\"AIRLINE\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution Analysis following Cleansing, Conversion, and preprocessing the features of data. Now we need to identify the features that are most likely to influence flight delays for our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dim = (9,10)\n",
    "f, ax = plt.subplots(figsize=fig_dim)\n",
    "quality=df_flights[\"AIRLINE\"].unique()\n",
    "size=df_flights[\"AIRLINE\"].value_counts()\n",
    "\n",
    "plt.pie(size,labels=quality,autopct='%1.0f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "fig_dim = (8,9)\n",
    "f, ax = plt.subplots(figsize=fig_dim)\n",
    "sns.despine(bottom=True, left=True)\n",
    "sns.stripplot(x=\"ARRIVAL_DELAY\", y=\"AIRLINE\",\n",
    "              data=df_flights, dodge=True, jitter=True\n",
    "            )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution above compares the airlines to their ARRIVAL DELAYs. It clearly demonstrates that American Airlines has a wide range of delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marking the delayed flights\n",
    "\n",
    "df_flights['DELAYED'] = df_flights.loc[:,'ARRIVAL_DELAY'].values > 0\n",
    "\n",
    "y = df_flights.DELAYED\n",
    "\n",
    "# Choosing the indeoendent variable\n",
    "feature_list_s = [\n",
    "    'LATE_AIRCRAFT_DELAY'\n",
    "    ,'AIRLINE_DELAY'\n",
    "    ,'AIR_SYSTEM_DELAY'\n",
    "    ,'WEATHER_DELAY'\n",
    "    ,'ELAPSED_TIME']\n",
    "\n",
    "X_small = df_flights[feature_list_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline-Model RandomForestClassifier \n",
    "clf = RandomForestClassifier(n_estimators = 10, random_state=32) \n",
    "clf.fit(X_small, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances=clf.feature_importances_\n",
    "importances=pd.DataFrame([X_small.columns,importances]).transpose()\n",
    "importances.columns=[['Variables','Importance']]\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing the predictors\n",
    "feature_list = [\n",
    "    'YEAR'\n",
    "    ,'MONTH'\n",
    "    ,'DAY'\n",
    "    ,'LATE_AIRCRAFT_DELAY'\n",
    "    ,'AIRLINE_DELAY'\n",
    "    ,'AIR_SYSTEM_DELAY'\n",
    "    ,'WEATHER_DELAY'\n",
    "    ,'ELAPSED_TIME'\n",
    "    ,'DEPARTURE_DELAY'\n",
    "    ,'SCHEDULED_TIME'\n",
    "    ,'AIR_TIME'\n",
    "    ,'DISTANCE'\n",
    "    ,'TAXI_IN'\n",
    "    ,'TAXI_OUT'\n",
    "    ,'DAY_OF_WEEK'\n",
    "    ,'SECURITY_DELAY'\n",
    "]\n",
    "# Any number can be used in place of '0'. \n",
    "import random\n",
    "random.seed(0)\n",
    "    \n",
    "df_flights_1=df_flights.sample(n=50000)\n",
    "X = df_flights_1[feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_flights_1.DELAYED\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "from sklearn.preprocessing import scale\n",
    "X_train=scale(X_train)\n",
    "X_test=scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLA = [ ensemble.AdaBoostRegressor(),\n",
    "    ensemble.BaggingRegressor(),\n",
    "    tree.DecisionTreeRegressor(),\n",
    "    tree.ExtraTreeRegressor(),\n",
    "    ensemble.ExtraTreesRegressor(),\n",
    "    ensemble.GradientBoostingRegressor(),\n",
    "    ensemble.RandomForestRegressor(),\n",
    "    neighbors.KNeighborsRegressor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLA_col = []\n",
    "MLA_compare = pd.DataFrame(columns = MLA_col)\n",
    "results=[]\n",
    "\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "    \n",
    "    cv_results = cross_val_score(alg, X_train, y_train, cv=10)\n",
    "    results.append(cv_results)\n",
    "    predicted = alg.fit(X_train, y_train).predict(X_test)\n",
    "    fp, tp, th = roc_curve(y_test, predicted)\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index,'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy'] = round(alg.score(X_train, y_train), 4)\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy'] = round(alg.score(X_test, y_test), 4)\n",
    "    MLA_compare.loc[row_index, 'MLA AUC'] = auc(fp, tp)\n",
    "    \n",
    "    \n",
    "    row_index+=1\n",
    "    \n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy'], ascending = False, inplace = True)    \n",
    "MLA_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below plot shows the test train accuracy with respect to each models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,6))\n",
    "sns.lineplot(x=\"MLA Name\", y=\"MLA Train Accuracy\",data=MLA_compare,palette='hot',label='Train Accuracy')\n",
    "sns.lineplot(x=\"MLA Name\", y=\"MLA Test Accuracy\",data=MLA_compare,palette='hot',label='Test Accuracy')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is basiccally the Test accuracy result of each models, where we can see Random Forest is giving us the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we analysed and predicted flight delays using Python and GridDB. We examined two ways to import our data, using (1) GridDB and (2) Pandas. For large datasets, GridDB provides an excellent alternative to import data in your notebook as it is open-source and highly scalable. [Download GridDB](https://griddb.net/en/downloads/) today!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
